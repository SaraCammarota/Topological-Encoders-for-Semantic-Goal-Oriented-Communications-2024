defaults:
  - dataset: mnist
  - my_model: model_config
  - dgm: topk_dgm_conf
  - pooling: topk_conf
  - training: train_conf
  - exp: compare_poolings
  - paths: default



hydra:
  sweeper:
    params:
      # pooling.pooling_type: choice('asa', 'sag', 'topk')
      # pooling.pooling_ratio: choice(0.1, 0.25, 0.5, 0.75, 0.99)
      # my_model.layers.n_pre: choice(3, 4)
      # my_model.layers.n_post: choice(2, 3)
      # my_model.layers.n_conv: choice(3, 4)
      # my_model.layers.n_dgm_layers: choice(3, 4)
      # training.lr: choice(0.001, 0.0001, 0.00001)   


# IMDB BINARY

## topk pooling 

### 0.1 
 #0 : my_model.layers.n_pre=3 my_model.layers.n_post=2 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3 training.lr=0.0001  0.531
 #4 : my_model.layers.n_pre=3 my_model.layers.n_post=2 my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=3 training.lr=0.0001  0.514
 #6 : my_model.layers.n_pre=3 my_model.layers.n_post=2 my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=4 training.lr=0.0001  0.499 sul train

### 0.25 
 #0 : my_model.layers.n_pre=3 my_model.layers.n_post=2 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3   0.532 (501 train)
 #3 : my_model.layers.n_pre=3 my_model.layers.n_post=2 my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=4   0.527 (497 sul train)
 #9 : my_model.layers.n_pre=4 my_model.layers.n_post=2 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=4    0.516 (513)

### 0.5    --- npre = 3, npost = 2 fixed
 #0 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                  0.542 (499 t)
 #3 : my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=4                  0.529 (512)

### 0.75
  #0 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                 0.526 (499)

### 0.99
  #0 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                 0.524 (510)
  #0 : my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=4                 0.514 (495)                 


## asa pooling 

### 0.1 
 #0 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                0.528 (533)
 #2 : my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=3              0.517 (526)

### 0.25 
#0 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                0.518 (517)
#1 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=4              0.527 (515)
#2 : my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=3              0.519 (513)

### 0.5    
#0 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                0.528 (511)
#1 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=4              0.521 (508)
#2 : my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=3              0.515 (509)

### 0.75


### 0.99


## sag pooling 

### 0.1 
 #0 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                0.507 (501)
      

### 0.25 
#0 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3                0.492 (501)              

### 0.5    
#0 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3              0.545 (499)
#1 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=4            0.505 (501)
#3 : my_model.layers.n_conv=4 my_model.layers.n_dgm_layers=4                        















# best configs with top-5 dgm:


#   best config previously found on mutag and topk pooling: val_loss, n_pre, n_post, n_conv, n_dgm_layers, lr   --->   0.347 2 1 2 2 5e-3

# with topk (and receiver gnn)
  #29 : my_model.layers.n_pre=2 my_model.layers.n_post=2 my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=3 my_model.layers.receiver=3  0.409


 #best config with sag on mutag
 #29 : my_model.layers.n_pre=2 my_model.layers.n_post=2 my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=1 training.lr=0.0005 0.361   
  #12 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3 my_model.layers.receiver=2 training.lr=0.001


 # with asa
   #25 : my_model.layers.n_pre=2 my_model.layers.n_post=2 my_model.layers.n_conv=1 my_model.layers.n_dgm_layers=1 training.lr=0.0005  0.386
   #4 : my_model.layers.n_pre=2 my_model.layers.n_post=2 my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=3 my_model.layers.receiver=2 training.lr=0.001   0.418


# best configs with alpha dgm


#12 : my_model.layers.n_pre=1 my_model.layers.n_post=2 my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=1 training.lr=0.001    0.317




# on imdb binary
  
  #topk pooling
  
  #29 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3 my_model.layers.receiver=3 my_model.dropout=0.1 training.lr=0.0005    0.529 

  #sag pooling

  #8 : my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=3 my_model.layers.receiver=2 my_model.dropout=0.1 training.lr=0.001     0.532

  #asa pooling

  #14 : my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=3 my_model.layers.receiver=3 training.lr=0.0005   0.525



# on mutag, without noise in training
# asa
#25 : my_model.layers.n_pre=2 my_model.layers.n_post=3 my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=2 training.lr=0.001
# sag
 #1 : my_model.layers.n_pre=2 my_model.layers.n_post=2 my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=2 training.lr=0.0005
# topk
#26 : my_model.layers.n_pre=3 my_model.layers.n_post=3 my_model.layers.n_conv=2 my_model.layers.n_dgm_layers=3 training.lr=0.001   282 95 80
#28 : my_model.layers.n_pre=3 my_model.layers.n_post=3 my_model.layers.n_conv=3 my_model.layers.n_dgm_layers=2 training.lr=0.001 328 95 81

# on proteins, without noise in training




# # # # # # # # # # # # BASELINE CONFIGURATION # # # # # # # # # # # #

baseline: 
  - dataset: 
  - my_model: kmeans
  - training: train_conf
  - exp: snr_conf
  - paths: default
